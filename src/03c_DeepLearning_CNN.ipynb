{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8fb28d8b",
      "metadata": {
        "id": "8fb28d8b"
      },
      "source": [
        "# Deep learning con Python\n",
        "## Redes Neuronales Convolucionales (CNN)\n",
        "\n",
        "En esta parte del taller, vamos a aprender a usar CNNs para reconocer objetos en imágenes urbanas (vehículos, árboles, etc.).\n",
        "\n",
        "Este notebook incluye:\n",
        "\n",
        "- Explicación introducctoria.\n",
        "- Preparación de dataset (COCO),\n",
        "- Dos modelos CNN (sencillo y más complejo).\n",
        "- Uso de una CNN ya pre-entrenada (YOLO, Ultralytics).\n",
        "- Visualización de las imágenes.\n",
        "- Ejercicios.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef490ff2",
      "metadata": {
        "id": "ef490ff2"
      },
      "source": [
        "## 0) ¿Qué es una CNN?\n",
        "\n",
        "- Capas convolucionales (filtros) que detectan patrones locales.\n",
        "- Pooling para reducir resolución y aportar invariancia local.\n",
        "- BatchNorm/Dropout para estabilizar/regularizar.\n",
        "\n",
        "**Tareas:** clasificación de imagen, detección (bounding boxes), segmentación (pixel-wise). En este taller haremos clasificación mediante crops y detección con YOLO."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display"
      ],
      "metadata": {
        "id": "ZwRQgpNSm1dp"
      },
      "id": "ZwRQgpNSm1dp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_cnn1.png\",width=800, height=300))"
      ],
      "metadata": {
        "id": "Z8TizCNE-jR1"
      },
      "id": "Z8TizCNE-jR1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_conv_operator.png\",width=800, height=400))"
      ],
      "metadata": {
        "id": "mkaH2Cb7KcH4"
      },
      "id": "mkaH2Cb7KcH4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_pooling.gif\",width=800, height=600))"
      ],
      "metadata": {
        "id": "u5t7XHL--_8z"
      },
      "id": "u5t7XHL--_8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y que se entiende por segmentación de imágenes?"
      ],
      "metadata": {
        "id": "hGHquFEKND0p"
      },
      "id": "hGHquFEKND0p"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_image_segmentation.png\",width=1000, height=600))"
      ],
      "metadata": {
        "id": "xtYieqj7NITw"
      },
      "id": "xtYieqj7NITw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ello vamos a seguir el mismo flujo de trabajo que ya usamos a la hora de desarrollar nuestras MLP y RNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "n4k54M1MabNh"
      },
      "id": "n4k54M1MabNh"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_pipeline.jpg\",width=800, height=300))"
      ],
      "metadata": {
        "id": "bX8PpGHcaaoI"
      },
      "id": "bX8PpGHcaaoI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "drnvRvXn16Hk"
      },
      "id": "drnvRvXn16Hk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Librerías Principales\n"
      ],
      "metadata": {
        "id": "yfRui0DqaPl7"
      },
      "id": "yfRui0DqaPl7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from pprint import pprint\n",
        "from PIL import Image as im\n",
        "from PIL import ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# reproducibilidad\n",
        "RND = 42\n",
        "np.random.seed(RND)"
      ],
      "metadata": {
        "id": "xrb_bd4HaPGM"
      },
      "id": "xrb_bd4HaPGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "P77lHzb4lnUq"
      },
      "id": "P77lHzb4lnUq"
    },
    {
      "cell_type": "markdown",
      "id": "76bd4b15",
      "metadata": {
        "id": "76bd4b15"
      },
      "source": [
        "## 2) Obtener el dataset de imágenes sobre el que vamos a trabajar (COCO)\n",
        "\n",
        "\n",
        "COCO (*Common Objects in Context*) es un dataset a gran escala para detección de objetos, segmentación y captioning. Contiene cientos de miles de imágenes y millones de instancias anotadas.\n",
        "\n",
        "COCO distribuye particiones (train/val/test) con anotaciones detalladas (bounding boxes, segmentaciones por instancia, categorías, keypoints para personas en algunos splits). Las anotaciones para train y val vienen en ficheros JSON como `instances_train2017.json` y `instances_val2017.json`.\n",
        "COCO Dataset\n",
        "\n",
        "En cuanto a su formato COCO es un único JSON con claves principales images, annotations, categories (entre otras). Cada annotation tiene campos como `bbox` (x,y,w,h), `segmentation` (si aplica), `category_id`, `area`, `iscrowd`, `id`.\n",
        "\n",
        "A continuación vamos a descargar dicho dataset y sus anotaciones."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'data'\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'val2017')          # carpeta con imágenes val2017\n",
        "ANN_FILE = os.path.join(DATA_DIR, 'annotations', 'instances_val2017.json')"
      ],
      "metadata": {
        "id": "tWVjIjkh8Ll3"
      },
      "id": "tWVjIjkh8Ll3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a689c580",
      "metadata": {
        "id": "a689c580"
      },
      "outputs": [],
      "source": [
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "!wget -nc -P {DATA_DIR} http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget -nc -P {DATA_DIR} http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -n {DATA_DIR}/val2017.zip -d {DATA_DIR}\n",
        "!unzip -n {DATA_DIR}/annotations_trainval2017.zip -d {DATA_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a visualizar el formado del dataset que hemos obtenido"
      ],
      "metadata": {
        "id": "pZ1Nw24D63NO"
      },
      "id": "pZ1Nw24D63NO"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(ANN_FILE, 'r', encoding='utf-8') as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "    # Información resumida\n",
        "    n_images = len(coco.get('images', []))\n",
        "    n_anns = len(coco.get('annotations', []))\n",
        "    n_cats = len(coco.get('categories', []))\n",
        "    print(f\"Resumen COCO (archivo {os.path.basename(ANN_FILE)}):\")\n",
        "    print(f\" - imágenes (registro JSON): {n_images}\")\n",
        "    print(f\" - anotaciones: {n_anns}\")\n",
        "    print(f\" - categorías: {n_cats}\")\n",
        "    print()\n",
        "\n",
        "    # Mostrar las primeras categorías (id:name)\n",
        "    cat_map = {c['id']: c['name'] for c in coco.get('categories', [])}\n",
        "    print(\"Algunas categorías (id -> name):\")\n",
        "    for cid in sorted(list(cat_map.keys()))[:20]:\n",
        "        print(f\"  {cid} -> {cat_map[cid]}\")\n"
      ],
      "metadata": {
        "id": "HsNkZ_cICpAj"
      },
      "id": "HsNkZ_cICpAj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ahora a mostrar una imágen y los objetos en ella contenidos."
      ],
      "metadata": {
        "id": "J48d7zPjEdjK"
      },
      "id": "J48d7zPjEdjK"
    },
    {
      "cell_type": "code",
      "source": [
        "img_rec = None\n",
        "ann_for_img= None\n",
        "\n",
        "with open(ANN_FILE, 'r', encoding='utf-8') as f:\n",
        "  coco = json.load(f)\n",
        "  img_rec = random.choice(coco['images'])\n",
        "  img_id = img_rec['id']\n",
        "  img_filename = img_rec.get('file_name', 'N/A')\n",
        "  img_path = os.path.join(IMAGES_DIR, img_filename)\n",
        "\n",
        "  print(\"Imagen elegida (registro JSON):\")\n",
        "  pprint(img_rec)\n",
        "  print()\n",
        "\n",
        "  if os.path.isfile(img_path):\n",
        "      # abrir y mostrar la imagen\n",
        "      img = im.open(img_path).convert('RGB')\n",
        "      plt.figure(figsize=(8,8))\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"ID {img_id}  —  {img_filename}\")\n",
        "      plt.show()\n",
        "  else:\n",
        "      print(\"No se encontró la imagen en disco:\", img_path)\n",
        "      print(\"Asegúrate de haber descomprimido val2017.zip en:\", IMAGES_DIR)\n",
        "      # no terminamos; aún mostramos las anotaciones (si existen en JSON)\n",
        "\n",
        "  # 4) Buscar anotaciones asociadas a esta imagen\n",
        "  anns_for_img = [a for a in coco['annotations'] if a['image_id'] == img_id]\n",
        "  print(f\"Se encontraron {len(anns_for_img)} anotaciones para la imagen (id={img_id}).\\n\")\n",
        "\n",
        "  # Mostrar (y explicar) las claves más importantes de la primera anotación\n",
        "  if len(anns_for_img) > 0:\n",
        "      print(\"Ejemplo de anotación (primera):\")\n",
        "      example_ann = anns_for_img[0]\n",
        "      # imprimimos los campos más relevantes\n",
        "      keys_of_interest = ['id','image_id','category_id','bbox','area','iscrowd','segmentation']\n",
        "      for k in keys_of_interest:\n",
        "          print(f\"  {k} :\", example_ann.get(k, None))\n",
        "      print(\"\\n(El bbox está en formato [x, y, width, height] con coordenadas en píxels.)\")\n",
        "\n",
        "      print(\"\\nTodas las anotaciones (resumen):\")\n",
        "      # imprimimos un resumen compacto de cada anotación: id, catname, bbox\n",
        "      for a in anns_for_img:\n",
        "          cid = a['category_id']\n",
        "          cname = cat_map.get(cid, str(cid))\n",
        "          bbox = a.get('bbox', None)\n",
        "          print(f\" - ann_id={a['id']:6d}  cat={cname:12s}  bbox={bbox}\")\n",
        "  else:\n",
        "      print(\"No hay anotaciones para esta imagen en el JSON.\")"
      ],
      "metadata": {
        "id": "A7asMG_g62uW"
      },
      "id": "A7asMG_g62uW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvamos a imprimir la imagen pero superponiendo los objetos etiquetados en el dataset"
      ],
      "metadata": {
        "id": "n3PIzZODFLDn"
      },
      "id": "n3PIzZODFLDn"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "# Mostrar la imagen con bounding boxes y etiquetas\n",
        "if os.path.isfile(img_path):\n",
        "    img = im.open(img_path).convert('RGB')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"ID {img_id}  —  {img_filename}\")\n",
        "\n",
        "    # Pintar cada anotación\n",
        "    for a in anns_for_img:\n",
        "        cid = a['category_id']\n",
        "        cname = cat_map.get(cid, str(cid))\n",
        "        bbox = a.get('bbox', None)  # formato [x, y, width, height]\n",
        "\n",
        "        if bbox:\n",
        "            x, y, w, h = bbox\n",
        "            # Añadir rectángulo\n",
        "            rect = patches.Rectangle(\n",
        "                (x, y), w, h,\n",
        "                linewidth=2,\n",
        "                edgecolor='red',\n",
        "                facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Etiqueta sobre el bbox\n",
        "            ax.text(\n",
        "                x, y - 5, cname,\n",
        "                fontsize=10,\n",
        "                color='white',\n",
        "                bbox=dict(facecolor='red', alpha=0.5, pad=1)\n",
        "            )\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No se encontró la imagen en disco:\", img_path)\n",
        "    print(\"Asegúrate de haber descomprimido val2017.zip en:\", IMAGES_DIR)\n"
      ],
      "metadata": {
        "id": "9CSVqX4n6ymf"
      },
      "id": "9CSVqX4n6ymf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "T6qxDo18lpBi"
      },
      "id": "T6qxDo18lpBi"
    },
    {
      "cell_type": "markdown",
      "id": "3535bfc9",
      "metadata": {
        "id": "3535bfc9"
      },
      "source": [
        "## 3) Pre-procesado básico\n",
        "\n",
        "Vamos a desarrollar una red convolucional que sepa detectar dada una imágen, si hay un vehículo en ella. Por tanto, nos enfrentamos a un problema de clasificacion binaria al iguial que vimos en el notebook de MLPs (coche vs no coche)\n",
        "\n",
        "Vamos extraer las imágenes de COCO cuyos bboxes sean `car`, `truck` o `bus` para generar ejemplos (`crops`) positivos. Para generar crops negativo, es decir, imágenes que no contengan ninguna de las etiquetas anteriores, vamos a extraer imágenes aleatorias bajo la etiqueta `background`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a341c6ad",
      "metadata": {
        "id": "a341c6ad"
      },
      "outputs": [],
      "source": [
        "out_dir = os.path.join(DATA_DIR, 'crops')\n",
        "\n",
        "with open(ANN_FILE, 'r') as f:\n",
        "  coco = json.load(f)\n",
        "  cat_map = {c['id']: c['name'] for c in coco['categories']}\n",
        "  vehicle_cat_ids = [cid for cid, name in cat_map.items() if name in ('car','truck','bus')]\n",
        "\n",
        "  shutil.rmtree(out_dir, ignore_errors=True)\n",
        "  os.makedirs(os.path.join(out_dir, 'vehicle'), exist_ok=True)\n",
        "  os.makedirs(os.path.join(out_dir, 'background'), exist_ok=True)\n",
        "\n",
        "  images_index = {img['id']: img for img in coco['images']}\n",
        "\n",
        "  MAX_VEHICLE = 2000\n",
        "  vehicle_count = 0\n",
        "  background_count = 0\n",
        "\n",
        "  for ann in tqdm(coco['annotations'], desc=\"Extrayendo imágenes con vehículos...\"):\n",
        "      if ann['category_id'] in vehicle_cat_ids and vehicle_count < MAX_VEHICLE:\n",
        "          img_info = images_index[ann['image_id']]\n",
        "          img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
        "          if not os.path.exists(img_path):\n",
        "              continue\n",
        "          img = im.open(img_path).convert('RGB')\n",
        "          x,y,w,h = ann['bbox']\n",
        "          left = max(0, int(x)); upper = max(0, int(y))\n",
        "          right = min(img.width, int(x+w)); lower = min(img.height, int(y+h))\n",
        "          if right-left <= 0 or lower-upper <= 0: continue\n",
        "          crop = img.crop((left,upper,right,lower)).resize((128,128))\n",
        "          crop.save(os.path.join(out_dir, 'vehicle', f'veh_{vehicle_count:05d}.jpg'))\n",
        "          vehicle_count += 1\n",
        "\n",
        "  random_images = random.sample(list(images_index.values()), min(1000, len(images_index)))\n",
        "  for img_info in tqdm(random_images, desc=\"Extrayendo imágenes con background \"):\n",
        "      img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
        "      if not os.path.exists(img_path): continue\n",
        "      img = im.open(img_path).convert('RGB')\n",
        "      for _ in range(3):\n",
        "          w = random.randint(64, 200)\n",
        "          h = random.randint(64, 200)\n",
        "          left = random.randint(0, max(0, img.width - w))\n",
        "          upper = random.randint(0, max(0, img.height - h))\n",
        "          crop = img.crop((left,upper,left+w,upper+h)).resize((128,128))\n",
        "          crop.save(os.path.join(out_dir, 'background', f'bg_{background_count:05d}.jpg'))\n",
        "          background_count += 1\n",
        "          if background_count >= vehicle_count and vehicle_count>0: break\n",
        "      if background_count >= vehicle_count and vehicle_count>0: break\n",
        "\n",
        "  print('\\nImágenes de vehículos', vehicle_count, 'Imágenes background', background_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7c0HsSUmHyQU"
      },
      "id": "7c0HsSUmHyQU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Particionado train/val/test\n",
        "\n",
        "La clase `ImageDataGenerator` de Keras es una herramienta muy práctica para preprocesar y aumentar (augment) imágenes antes de entrenar una red neuronal convolucional (CNN). Esta clase permite:\n",
        "\n",
        "- Escala los píxeles a un rango determinado (ej. [0,1] si usas rescale=1./255).\n",
        "\n",
        "- Permite normalizar, centrar o estandarizar imágenes.\n",
        "\n",
        "- Aplica transformaciones aleatorias en cada época de entrenamiento (cuando se usa data augmentation).\n",
        "\n",
        "- Generación de lotes (batches)\n",
        "\n",
        "- No carga todas las imágenes en memoria.\n",
        "\n",
        "- Genera batches de imágenes en tiempo real mientras entrenas (fit o fit_generator).\n",
        "\n",
        "- Es eficiente para trabajar con datasets grandes.\n",
        "\n",
        "Además permite realizar *Data Augmentation* (aumento de datos) ¿Y eso qué es\n",
        "\n",
        "- Genera versiones modificadas de las imágenes originales para que el modelo generalice mejor.\n",
        "\n",
        "Ejemplos de transformaciones:\n",
        "\n",
        "- Rotaciones (rotation_range)\n",
        "\n",
        "- Traslaciones horizontales y verticales (width_shift_range, height_shift_range)\n",
        "\n",
        "- Zoom (zoom_range)\n",
        "\n",
        "- Volteos horizontales o verticales (horizontal_flip, vertical_flip)\n",
        "\n",
        "- Brillo o contraste (brightness_range)\n",
        "\n",
        "- Cortes y recortes aleatorios\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "naLKNDtOZb4J"
      },
      "id": "naLKNDtOZb4J"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_CROPS = os.path.join('data','crops')\n",
        "img_size = (128,128)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2, horizontal_flip=True)\n",
        "train_gen = train_datagen.flow_from_directory(DATA_CROPS, target_size=img_size, batch_size=batch_size, subset='training', class_mode='binary')\n",
        "val_gen = train_datagen.flow_from_directory(DATA_CROPS, target_size=img_size, batch_size=batch_size, subset='validation', class_mode='binary')"
      ],
      "metadata": {
        "id": "LX_SVymSZRwY"
      },
      "id": "LX_SVymSZRwY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1-KU9uN-e3DL"
      },
      "id": "1-KU9uN-e3DL"
    },
    {
      "cell_type": "markdown",
      "id": "2b5d224c",
      "metadata": {
        "id": "2b5d224c"
      },
      "source": [
        "## 5)  Nuestra primera Red Neuronal Convolucional (CNN) con Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_simple_cnn():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(*img_size,3)),\n",
        "        layers.Conv2D(16, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(32, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, activation='relu'),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "9bQqDm3UXTyH"
      },
      "id": "9bQqDm3UXTyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a contruir nuestra CNN y ver un resumen de sus parámetros"
      ],
      "metadata": {
        "id": "fCIFPLzDXdDW"
      },
      "id": "fCIFPLzDXdDW"
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model = build_simple_cnn()\n",
        "simple_model.summary()"
      ],
      "metadata": {
        "id": "d_Kr3995XaQn"
      },
      "id": "d_Kr3995XaQn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  history_simple = simple_model.fit(train_gen, epochs=5, validation_data=val_gen)"
      ],
      "metadata": {
        "id": "Sj-CVI6TYeat"
      },
      "id": "Sj-CVI6TYeat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como ya sabemos mostramos las curvas de aprendizaje"
      ],
      "metadata": {
        "id": "0kDUfjzvYUOo"
      },
      "id": "0kDUfjzvYUOo"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history_simple.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history_simple.history['val_loss'], label='Validacion')\n",
        "plt.legend(); plt.title('Loss')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history_simple.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history_simple.history['val_accuracy'], label='Validación')\n",
        "plt.legend(); plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uu9HvDUYRUg"
      },
      "id": "6uu9HvDUYRUg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a visualizar algunos de los resultados obtenidos por nuestra primera CNN"
      ],
      "metadata": {
        "id": "ahBQ0hjPkaII"
      },
      "id": "ahBQ0hjPkaII"
    },
    {
      "cell_type": "code",
      "source": [
        "crops_veh_dir = os.path.join('data','crops','vehicle')\n",
        "files = [f for f in os.listdir(crops_veh_dir) if f.lower().endswith('.jpg')]\n",
        "for f in files[:3]:\n",
        "    example = os.path.join(crops_veh_dir,f)\n",
        "    img = im.open(example).convert('RGB').resize((128,128))\n",
        "    display(img)\n",
        "    try:\n",
        "        x = np.array(img)/255.0\n",
        "        pred = simple_model.predict(x[None,...])[0,0]\n",
        "        print('Probabilidad de vehiculo:', float(pred))\n",
        "    except Exception as e:\n",
        "        print('Modelo  no disponible en memoria. Entrena el modelo antes.')\n"
      ],
      "metadata": {
        "id": "qsF1hZ5Zi_kc"
      },
      "id": "qsF1hZ5Zi_kc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crops_bk_dir = os.path.join('data','crops','background')\n",
        "files = [f for f in os.listdir(crops_bk_dir) if f.lower().endswith('.jpg')]\n",
        "for f in files[:3]:\n",
        "    example = os.path.join(crops_bk_dir,f)\n",
        "    img = im.open(example).convert('RGB').resize((128,128))\n",
        "    display(img)\n",
        "    try:\n",
        "        x = np.array(img)/255.0\n",
        "        pred = simple_model.predict(x[None,...])[0,0]\n",
        "        print('Probabilidad de vehiculo:', float(pred))\n",
        "    except Exception as e:\n",
        "        print('Modelo no disponible en memoria. Entrena el modelo antes.')"
      ],
      "metadata": {
        "id": "Hm09bDuBkZQQ"
      },
      "id": "Hm09bDuBkZQQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7543788e",
      "metadata": {
        "id": "7543788e"
      },
      "source": [
        "### CNN más compleja\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_complex_cnn():\n",
        "    inp = layers.Input(shape=(*img_size,3))\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Fk-L6Ls8fBO5"
      },
      "id": "Fk-L6Ls8fBO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complex_model = build_complex_cnn()\n",
        "complex_model.summary()"
      ],
      "metadata": {
        "id": "nzvxHHa7fVkZ"
      },
      "id": "nzvxHHa7fVkZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_complex = complex_model.fit(train_gen, epochs=5, validation_data=val_gen)"
      ],
      "metadata": {
        "id": "VYi0EsEYfXsY"
      },
      "id": "VYi0EsEYfXsY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos sus curvas de aprendizaje"
      ],
      "metadata": {
        "id": "wERbq6Mdmdyi"
      },
      "id": "wERbq6Mdmdyi"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history_complex.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history_complex.history['val_loss'], label='Validación')\n",
        "plt.legend(); plt.title('Loss')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history_complex.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history_complex.history['val_accuracy'], label='Validación')\n",
        "plt.legend(); plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eYxBguLlfaRP"
      },
      "id": "eYxBguLlfaRP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crops_veh_dir = os.path.join('data','crops','vehicle')\n",
        "files = [f for f in os.listdir(crops_veh_dir) if f.lower().endswith('.jpg')]\n",
        "for f in files[:3]:\n",
        "    example = os.path.join(crops_veh_dir,f)\n",
        "    img = im.open(example).convert('RGB').resize((128,128))\n",
        "    display(img)\n",
        "    try:\n",
        "        x = np.array(img)/255.0\n",
        "        pred = complex_model.predict(x[None,...])[0,0]\n",
        "        print('Probabilidad de vehiculo:', float(pred))\n",
        "    except Exception as e:\n",
        "        print('Modelo complejo no disponible en memoria. Entrena el modelo antes.')"
      ],
      "metadata": {
        "id": "VEYfuALfk7FP"
      },
      "id": "VEYfuALfk7FP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crops_bk_dir = os.path.join('data','crops','background')\n",
        "files = [f for f in os.listdir(crops_bk_dir) if f.lower().endswith('.jpg')]\n",
        "for f in files[:3]:\n",
        "    example = os.path.join(crops_bk_dir,f)\n",
        "    img = im.open(example).convert('RGB').resize((128,128))\n",
        "    display(img)\n",
        "    try:\n",
        "        x = np.array(img)/255.0\n",
        "        pred = complex_model.predict(x[None,...])[0,0]\n",
        "        print('Probabilidad de vehiculo:', float(pred))\n",
        "    except Exception as e:\n",
        "        print('Modelo no disponible en memoria. Entrena el modelo antes.')"
      ],
      "metadata": {
        "id": "5mcAVWramXb5"
      },
      "id": "5mcAVWramXb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9927e556",
      "metadata": {
        "id": "9927e556"
      },
      "source": [
        "### Uso de detector pre-entrenado: YOLO\n",
        "\n",
        "Afortunadamente, existen muchas CNNs pre-entrenadas que podemos usar para analizar los datos.\n",
        "\n",
        "En nuestro caso vamos a usar, YOLO (You Only Look Once) es una arquitectura de red neuronal convolucional diseñada para detectar objetos en imágenes en tiempo real. A diferencia de métodos clásicos (como R-CNN y sus variantes), YOLO aborda la detección como un único problema de regresión, prediciendo directamente en una sola pasada:\n",
        "\n",
        "- Las cajas delimitadoras (bounding boxes).\n",
        "\n",
        "- Las clases de los objetos.\n",
        "\n",
        "- Las confianzas de cada predicción.\n",
        "\n",
        "Características principales\n",
        "\n",
        "- Velocidad: es muy rápido, adecuado para aplicaciones en tiempo real (cámaras de tráfico, vigilancia, coches autónomos).\n",
        "\n",
        "- Detección múltiple: identifica múltiples objetos en una misma imagen con sus bounding boxes y categorías.\n",
        "\n",
        "- División en celdas: la imagen se divide en una cuadrícula, y cada celda predice cajas y probabilidades de clase.\n",
        "\n",
        "- Precisión vs. velocidad: versiones modernas (YOLOv5, YOLOv8) logran un buen equilibrio entre exactitud y rapidez."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_yolo.jpg\",width=800, height=900))"
      ],
      "metadata": {
        "id": "26au_lwd_TOD"
      },
      "id": "26au_lwd_TOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a instalar la librería que habilita el uso de dicha CNN."
      ],
      "metadata": {
        "id": "joQJuBWUBVD6"
      },
      "id": "joQJuBWUBVD6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "AqVwPPx681eb"
      },
      "id": "AqVwPPx681eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a usar ahora YOLO para detectar objetos, en concreto vehículos sobre el dataset COCO que hemos estado usando."
      ],
      "metadata": {
        "id": "51tCaMeJI0Xh"
      },
      "id": "51tCaMeJI0Xh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba13a35",
      "metadata": {
        "id": "6ba13a35"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "sample_img = None\n",
        "\n",
        "\n",
        "with open(ANN_FILE, 'r') as f:\n",
        "  coco = json.load(f)\n",
        "  cat_map = {c['id']: c['name'] for c in coco['categories']}\n",
        "  vehicle_cat_ids = [cid for cid, name in cat_map.items() if name in ('car','truck','bus','motorcycle')]\n",
        "\n",
        "  images_index = {img['id']: img for img in coco['images']}\n",
        "\n",
        "  vehicle_count=0\n",
        "  for ann in tqdm(coco['annotations'], desc=\"Analizando imágenes con vehículos...\"):\n",
        "      if ann['category_id'] in vehicle_cat_ids and vehicle_count < 10:\n",
        "          img_info = images_index[ann['image_id']]\n",
        "          img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
        "          if not os.path.exists(img_path):\n",
        "              continue\n",
        "          results = model.predict(source=img_path, conf=0.25)\n",
        "          img = im.open(img_path).convert('RGB')\n",
        "          draw = ImageDraw.Draw(img)\n",
        "          for r in results:\n",
        "              if hasattr(r, 'boxes') and r.boxes is not None:\n",
        "                for box in r.boxes:\n",
        "                    cls_id = int(box.cls[0].item())         # id de clase\n",
        "                    cls_name = model.names[cls_id]          # nombre de la clase\n",
        "                    #print(cls_id, cls_name)\n",
        "\n",
        "                boxes = r.boxes.xyxy.cpu().numpy()\n",
        "\n",
        "                for i in range(len(r.boxes)):\n",
        "                  cls_id = int(r.boxes[i].cls[0].item())         # id de clase\n",
        "                  cls_name = model.names[cls_id]\n",
        "\n",
        "                  if cls_name in 'car truck bus motorcycle'.split():\n",
        "                    x1,y1,x2,y2 = boxes[i][:4]\n",
        "                    draw.rectangle([x1,y1,x2,y2], outline='red', width=3)\n",
        "\n",
        "          display(img)\n",
        "          vehicle_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Yg0Z0NRZk57K"
      },
      "id": "Yg0Z0NRZk57K"
    },
    {
      "cell_type": "markdown",
      "id": "d3399e1b",
      "metadata": {
        "id": "d3399e1b"
      },
      "source": [
        "## 6) Conclusiones\n",
        "\n",
        "- Las CNNs son efectivas para extraer características jerárquicas de imágenes.\n",
        "- Para detección en escenas urbanas, usar detectores (YOLO, Faster-RCNN) permite localizar múltiples instancias; entrenar clasificadores con crops es útil pedagógicamente.\n",
        "- En un taller: usar subconjuntos y modelos pequeños (`yolov8n`) para tiempos razonables."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "XpGeLdPxHs8F"
      },
      "id": "XpGeLdPxHs8F"
    },
    {
      "cell_type": "markdown",
      "id": "d47afde2",
      "metadata": {
        "id": "d47afde2"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "1. Modifica el pipeline de crops para añadir `person` y entrena un clasificador ternario (vehicle, person, background).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Eso es todo amigos!"
      ],
      "metadata": {
        "id": "Pl9y0fFPHuXm"
      },
      "id": "Pl9y0fFPHuXm"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}