{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9ded34f",
      "metadata": {
        "id": "f9ded34f"
      },
      "source": [
        "# Deep Learning con Python\n",
        "\n",
        "## Multilayer Perceptron (MLP)\n",
        "\n",
        "En esta parte del taller construiremos y entrenaremos perceptrones multicapa (MLP) usando `tensorflow.keras` para resolver un problema de clasificación tabular relacionado con datos urbanos.\n",
        "\n",
        "En concreto, **vamos a implementar una MLP para detectar si el nivel de polución en un área de una ciudad es alto o no en base a diferentes variables climátológicas, poblacionales y de uso del terreno urbano**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Librería para poder visualizar imágenes\n",
        "from IPython.display import Image, display"
      ],
      "metadata": {
        "id": "3usEogIimWv6"
      },
      "id": "3usEogIimWv6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "lnCji9Kzn_uj"
      },
      "id": "lnCji9Kzn_uj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Flujo de trabajo\n",
        "\n",
        "Para desarrollar desde 0 un algoritmo de Deep Learning capaz de procesar una serie de datos de entrada y proporcionar un servicio *inteligente* generalmente se sigue un flujo de trabajo definodo por 5 pasos concretos."
      ],
      "metadata": {
        "id": "Cs5F78ofoXVt"
      },
      "id": "Cs5F78ofoXVt"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_pipeline.jpg\",width=800, height=300))"
      ],
      "metadata": {
        "id": "k5dsAEnFmZsn"
      },
      "id": "k5dsAEnFmZsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "25d04eb1",
      "metadata": {
        "id": "25d04eb1"
      },
      "source": [
        "## 1) Librerías principales\n",
        "\n",
        "Breve descripción de las librerías que usaremos:\n",
        "\n",
        "- `pandas`, `numpy`: manejo y generación de datos.\n",
        "- `matplotlib`, `seaborn`: visualización.\n",
        "- `scikit-learn`: creación de dataset sintético, particionado, métricas y escalado.\n",
        "- `tensorflow.keras`: definición, entrenamiento y evaluación de MLPs.\n",
        "\n",
        "Ejecuta la siguiente celda para importar librerías."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df75e324",
      "metadata": {
        "id": "df75e324"
      },
      "outputs": [],
      "source": [
        "# Imports básicos (ejecutar en el notebook)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# reproducibilidad\n",
        "RND = 42\n",
        "np.random.seed(RND)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7fd53b",
      "metadata": {
        "id": "0c7fd53b"
      },
      "source": [
        "## 2) Lectura del dataset\n",
        "\n",
        "En primer lugar vamos a leer el dataset con el que debemos trabajar. Puesto que dicho fichero se encuentra en formato CSV, vamos a hacer uso del método `read_csv` que aprendimos a usar en el notebook de `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer el dataset desde CSV\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/refs/heads/main/datos/city_mlp_dataset.csv\")\n",
        "if 'high_pollution' in df.columns:\n",
        "    df['high_pollution'] = df['high_pollution'].astype(int)\n",
        "\n",
        "print('Dataset cargado. Shape:', df.shape)"
      ],
      "metadata": {
        "id": "D0jix8WmenHh"
      },
      "id": "D0jix8WmenHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ca4ad687",
      "metadata": {
        "id": "ca4ad687"
      },
      "source": [
        "## 3) Análisis exploratorio de datos (sencillo)\n",
        "\n",
        "Una vez que ya tenemos acceso a los datos con los que vamos a trabajar, en primer lugar es necesario familiarizarnos con los mismos e intentar comprenderlos bien. Esto se hace generalmente visualizando los datos e intentando computar determinádas estadísticas del dataset. Todo ello se suele denominar *Análisis Exploratorio de los Datos*.\n",
        "\n",
        "Vamos a ver las primeras filas, descripción estadística y conteo de nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b5f7ff",
      "metadata": {
        "id": "a6b5f7ff"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nDescripción numérica:')\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "7d6jx97Zfx5y"
      },
      "id": "7d6jx97Zfx5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valores nulos\n",
        "print('\\nValores nulos por columna:')\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "4jU2-OYBfy_B"
      },
      "id": "4jU2-OYBfy_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a visualizar alguna fila en concreto que tenga valores NaN"
      ],
      "metadata": {
        "id": "a9-HLP91qrja"
      },
      "id": "a9-HLP91qrja"
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar filas que contienen al menos un NaN\n",
        "filas_con_nan = df[df.isna().any(axis=1)]\n",
        "\n",
        "# Mostrar solo 3 filas (si hay más, toma las primeras 3)\n",
        "print(\"\\nTres filas con al menos un NaN:\")\n",
        "display(filas_con_nan.head(3))"
      ],
      "metadata": {
        "id": "_nO8uuY3qpez"
      },
      "id": "_nO8uuY3qpez",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb46a96c",
      "metadata": {
        "id": "eb46a96c"
      },
      "source": [
        "### Imputación de NaNs\n",
        "\n",
        "Imputaremos los `NaN` de las columnas de atributos con la media. Nota de buenas prácticas: en un flujo real se debe calcular la imputación sólo en el conjunto de entrenamiento y aplicar esos valores al resto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7011db",
      "metadata": {
        "id": "2f7011db"
      },
      "outputs": [],
      "source": [
        "# Imputar NaNs con la media (solo features)\n",
        "feature_cols = df.columns.drop('high_pollution')\n",
        "nans_before = df[feature_cols].isnull().sum().sum()\n",
        "print(f'NaNs totales antes: {nans_before}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in feature_cols:\n",
        "    if df[col].isnull().any():\n",
        "        mean_val = df[col].mean()\n",
        "        df.fillna({col:mean_val}, inplace=True)\n",
        "nans_after = df[feature_cols].isnull().sum().sum()\n",
        "print(f'NaNs totales después: {nans_after}')"
      ],
      "metadata": {
        "id": "vzPr-abpg7nZ"
      },
      "id": "vzPr-abpg7nZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wLgBTfs0q5do"
      },
      "id": "wLgBTfs0q5do",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22391ba6",
      "metadata": {
        "id": "22391ba6"
      },
      "source": [
        "### Visualizaciones rápidas: histogramas y matriz de correlación\n",
        "\n",
        "Vamos a hacer una visualización sencilla de cómo están distribuidos los datos con los que vamos a trabajar mediante un gráfico de histogramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e081da",
      "metadata": {
        "id": "22e081da"
      },
      "outputs": [],
      "source": [
        "# histogramas\n",
        "plt.figure(figsize=(12,8))\n",
        "df[feature_cols].hist(figsize=(12,8))\n",
        "plt.suptitle('Histogramas de atributos (inspección rápida)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora cuál es el coefficiente de correlación de Pearson entre cada par de variables."
      ],
      "metadata": {
        "id": "NPUFb6R5seQv"
      },
      "id": "NPUFb6R5seQv"
    },
    {
      "cell_type": "code",
      "source": [
        "# correlación\n",
        "plt.figure(figsize=(8,6))\n",
        "cor = df.corr()\n",
        "sns.heatmap(cor, annot=True, fmt='.2f')\n",
        "plt.title('Matriz de correlación')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9gb7dtxJsGpk"
      },
      "id": "9gb7dtxJsGpk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cdffdb00",
      "metadata": {
        "id": "cdffdb00"
      },
      "source": [
        "## 4) Particionado train/val/test y escalado\n",
        "\n",
        "Cuando trabajamos con modelos de aprendizaje automático, es fundamental dividir los datos en conjuntos separados para asegurar que el modelo aprenda correctamente y podamos evaluar su desempeño de manera objetiva.\n",
        "\n",
        "- Train (entrenamiento): es la parte más grande del dataset y se usa para que el modelo aprenda los patrones de los datos.\n",
        "\n",
        "- Validation (validación): se utiliza durante el proceso de entrenamiento para ajustar los hiperparámetros y evitar el sobreajuste (overfitting). Sirve como una referencia intermedia para ver qué tan bien generaliza el modelo antes de probarlo con datos completamente nuevos.\n",
        "\n",
        "- Test o *Hold out* (prueba): es un conjunto de datos separado, que no se ha usado ni en el entrenamiento ni en la validación. Se emplea al final para medir el rendimiento real del modelo sobre datos nunca vistos.\n",
        "\n",
        "En resumen: train enseña al modelo, validation guía el ajuste, y test mide la capacidad de generalización.-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/train_val_test_split.png\",width=900, height=250))"
      ],
      "metadata": {
        "id": "NMc2qB9ftR0L"
      },
      "id": "NMc2qB9ftR0L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Separamos en 60% train, 20% val, 20% test. Escalamos usando `StandardScaler` ajustado sólo con train."
      ],
      "metadata": {
        "id": "l1H4u4sLtxV3"
      },
      "id": "l1H4u4sLtxV3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39cd4b81",
      "metadata": {
        "id": "39cd4b81"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['high_pollution']).values\n",
        "y = df['high_pollution'].values\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=RND)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=RND)\n",
        "\n",
        "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando entrenamos una red neuronal MLP para trabajar con datos tabulares, es muy importante que las variables de entrada estén en escalas similares. Esto se debe a que:\n",
        "\n",
        "- Diferencias de escala: En un conjunto de datos puede haber columnas con valores muy grandes (ej. ingresos anuales en miles de euros) y otras con valores muy pequeños (ej. edad en años). Si no normalizamos, la red tenderá a dar más importancia a las variables con números grandes, aunque no sean realmente más relevantes.\n",
        "\n",
        "- Velocidad de entrenamiento: Los algoritmos de optimización (como Adam o SGD) funcionan mejor cuando los datos están centrados y con una escala similar. De lo contrario, el proceso de ajuste de pesos puede ser lento e ineficiente.\n",
        "\n",
        "- Estabilidad numérica: Si los valores son demasiado grandes o demasiado pequeños, las funciones de activación (como sigmoid o tanh) pueden saturarse, lo que provoca que los gradientes se vuelvan casi nulos y la red aprenda muy poco.\n",
        "\n",
        "Por eso se usa el `StandardScaler`, que transforma cada variable restando su media y dividiéndola por su desviación estándar. Como resultado, cada característica tiene media ≈ 0 y desviación estándar ≈ 1.\n",
        "\n",
        "De esta manera, la MLP recibe todas las variables en una escala comparable, lo que mejora la eficiencia del entrenamiento y aumenta la precisión del modelo."
      ],
      "metadata": {
        "id": "ldpXImoZFd5M"
      },
      "id": "ldpXImoZFd5M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "EWphCdTeFdod"
      },
      "id": "EWphCdTeFdod",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled[:4]"
      ],
      "metadata": {
        "id": "AEb_11DmFrtc"
      },
      "id": "AEb_11DmFrtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:4]"
      ],
      "metadata": {
        "id": "ssTinBotF2Gr"
      },
      "id": "ssTinBotF2Gr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ba9e2ef7",
      "metadata": {
        "id": "ba9e2ef7"
      },
      "source": [
        "## 5) Nuestra primera Red Neuronal Artificial con Keras\n",
        "\n",
        "### ¿Qué es Keras?\n",
        "\n",
        "Keras es una **biblioteca de código abierto en Python** diseñada para facilitar la creación y el entrenamiento de modelos de **redes neuronales**.\n",
        "\n",
        "### Características principales\n",
        "- **Alto nivel de abstracción**: permite construir modelos de *deep learning* con pocas líneas de código.  \n",
        "- **Integración con TensorFlow**: desde 2019 es la API oficial de alto nivel (`tf.keras`).  \n",
        "- **Rapidez en prototipado**: útil para probar ideas nuevas rápidamente.  \n",
        "- **Flexibilidad**: admite configuraciones avanzadas cuando se necesita.  \n",
        "- **Ecosistema completo**: incluye módulos para redes densas, convolucionales, recurrentes, embeddings, dropout, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conceptos básicos en entrenamiento de redes neuronales\n",
        "\n",
        "- **Epoch**: una pasada completa por todos los datos de entrenamiento.  \n",
        "- **Batch size**: número de muestras procesadas antes de actualizar los parámetros del modelo.  \n",
        "- **Learning rate**: tamaño del paso que da el algoritmo al ajustar los pesos en cada actualización.  \n",
        "- **Accuracy**: métrica que mide el porcentaje de predicciones correctas hechas por el modelo.  \n"
      ],
      "metadata": {
        "id": "IM4pklJMlnRX"
      },
      "id": "IM4pklJMlnRX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Red Neuronal Multilayer Preceptron (MLP)"
      ],
      "metadata": {
        "id": "Biz4xQ4-mPeI"
      },
      "id": "Biz4xQ4-mPeI"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_mlp_arch.jpg\",width=1000, height=500))"
      ],
      "metadata": {
        "id": "SSmZOecDmzEn"
      },
      "id": "SSmZOecDmzEn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el código que permita definir nuestra MLP. En este caso, las capas serán:\n",
        "\n",
        "- 1 de entrada (`Input`)\n",
        "- 1 oculta (`Dense`)\n",
        "- 1 de salida con una única neurona (`Dense`)\n",
        "\n",
        "Es importante destacar porqué usamos la funcion sigmoide (`sigmoid`) en la capa de salida. En problemas de clasificación binaria, la función *sigmoide* se utiliza en la capa de salida por varios motivos:\n",
        "\n",
        "- Convierte la salida del modelo en un valor entre 0 y 1.  \n",
        "- Ese valor puede interpretarse como la *probabilidad* de pertenecer a la clase positiva (clase 1).  \n",
        "- Permite tomar decisiones aplicando un umbral (por defecto, 0.5):  \n",
        "  - Si la salida ≥ 0.5 → se predice la clase 1.  \n",
        "  - Si la salida < 0.5 → se predice la clase 0.  \n"
      ],
      "metadata": {
        "id": "N4uBsW08oAne"
      },
      "id": "N4uBsW08oAne"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_funcion_sigmoide.png\",width=400, height=400))"
      ],
      "metadata": {
        "id": "G9vA0JbhpLRm"
      },
      "id": "G9vA0JbhpLRm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_mlp_simple(input_dim, lr=1e-3):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZT49go4Pn-EQ"
      },
      "id": "ZT49go4Pn-EQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = construir_mlp_simple(X_train_scaled.shape[1], lr=1e-3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "u777Mu4Xow3-"
      },
      "id": "u777Mu4Xow3-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_val_scaled, y_val),\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    verbose=2)"
      ],
      "metadata": {
        "id": "4_r96mqIo2Pe"
      },
      "id": "4_r96mqIo2Pe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning curves en el entrenamiento de una red neuronal\n",
        "\n",
        "Las curvas de aprendizaje son gráficos que muestran cómo evoluciona el rendimiento de un modelo a lo largo del entrenamiento.  \n",
        "Normalmente se representan dos curvas:  \n",
        "- *Pérdida (loss)* en entrenamiento y validación.  \n",
        "- *Métrica de calidad* (en nuestro caso, accuracy) en entrenamiento y validación.  \n",
        "\n",
        "#### ¿Por qué son importantes?\n",
        "Revisarlas permite:\n",
        "- Detectar *overfitting*: cuando el rendimiento en entrenamiento mejora pero en validación empeora.  \n",
        "- Detectar *underfitting*: cuando el modelo no logra un buen rendimiento ni en entrenamiento ni en validación.  \n",
        "- Ajustar *hiperparámetros* como número de *epochs*, *learning rate* o complejidad de la red.  \n",
        "- Verificar si el modelo *sigue aprendiendo* o si ya se ha estancado.  \n",
        "\n"
      ],
      "metadata": {
        "id": "34nnoh81pa3Y"
      },
      "id": "34nnoh81pa3Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Curvas\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJh7oplMpZB-"
      },
      "id": "BJh7oplMpZB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación en test\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"Test loss: {test_loss:.4f}  |  Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "yDBpCLeKqUGH"
      },
      "id": "yDBpCLeKqUGH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_accuracy_recall.png\",width=350, height=400))"
      ],
      "metadata": {
        "id": "hINwylPDuY0f"
      },
      "id": "hINwylPDuY0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4118147",
      "metadata": {
        "id": "e4118147"
      },
      "outputs": [],
      "source": [
        "# Reporte\n",
        "y_pred_prob = model.predict(X_test_scaled).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nConfusion matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "U5QcJIyevKsX"
      },
      "id": "U5QcJIyevKsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b4a51fc4",
      "metadata": {
        "id": "b4a51fc4"
      },
      "source": [
        "## 6) Variantes y EarlyStopping\n",
        "\n",
        "Vamos a probar algunas variantes de MLP añadiendole algunas capas ocultas más y cambiando su número de neuronas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_variante_mlp(input_dim, hidden_units, lr=1e-3):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "    for units in hidden_units:\n",
        "        model.add(layers.Dense(units, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "4nEBYMTHq28f"
      },
      "id": "4nEBYMTHq28f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También vamos a usar `early stopping`, una técnica de regularización que consiste en detener el entrenamiento de una red neuronal antes de que termine el número máximo de *epochs* programadas.  \n",
        "\n",
        "Durante el entrenamiento se monitoriza una métrica en el conjunto de validación.  Si esa métrica deja de mejorar después de cierto número de *epochs consecutivas* (llamado *patience*), el entrenamiento se interrumpe automáticamente.  \n",
        "\n",
        "De esta forma, se evita el *overfitting*, ya que el modelo deja de entrenar justo antes de empezar a memorizar demasiado los datos de entrenamiento, ahorra tiempo y recursos de cómputo y suele producir un modelo con mejor capacidad de generalización en datos nuevos.  \n",
        "\n"
      ],
      "metadata": {
        "id": "vLlysBuWrd6W"
      },
      "id": "vLlysBuWrd6W"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(url=\"https://raw.githubusercontent.com/fterroso/curso_ia_smart_cities/main/img/ml_early_stopping.png\",width=400, height=380))"
      ],
      "metadata": {
        "id": "LxkFA7jasc02"
      },
      "id": "LxkFA7jasc02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78114155",
      "metadata": {
        "id": "78114155"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "variants = {\n",
        "    'A_64_32': [64,32],\n",
        "    'B_128_64_32': [128,64,32]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, hidden in variants.items():\n",
        "    print('\\n=== Entrenando variante', name, 'con capas', hidden, '===')\n",
        "    model_var = construir_variante_mlp(X_train_scaled.shape[1], hidden, lr=1e-3)\n",
        "    early = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "    hist = model_var.fit(X_train_scaled, y_train,\n",
        "                         validation_data=(X_val_scaled, y_val),\n",
        "                         epochs=100,\n",
        "                         batch_size=32,\n",
        "                         callbacks=[early],\n",
        "                         verbose=2)\n",
        "    loss, acc = model_var.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    y_p = (model_var.predict(X_test_scaled).ravel() >= 0.5).astype(int)\n",
        "    rep = classification_report(y_test, y_p)\n",
        "    results[name] = {\n",
        "        'model': model_var,\n",
        "        'history': hist,\n",
        "        'test_loss': loss,\n",
        "        'test_acc': acc,\n",
        "        'report': rep\n",
        "    }\n",
        "    print(f\"Variante {name} -> Test acc: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen\n",
        "\n",
        "for name, info in results.items():\n",
        "    print(name)\n",
        "    print(info['report'])\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "uBMm_cH3HvJA"
      },
      "id": "uBMm_cH3HvJA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75b9f260",
      "metadata": {
        "id": "75b9f260"
      },
      "source": [
        "¡Eso es todo amigos!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}